 ##Measures of center
  #median
  np.median(msleep["sleep_total"]
  
  #mode
  import statistics
  statistics.mode(msleep["vore'])
  
  #adding an outlier
  msleep[msleep['vore'] == 'insecti']['sleep_total'].agg([np.mean, np.median])
  
  
  
  #Mean and Median
  # Import numpy with alias np
  import numpy as np

  # Filter for Belgium
  be_consumption = food_consumption[food_consumption["country"] == 'Belgium']

  # Filter for USA
  usa_consumption = food_consumption[food_consumption["country"] == "USA"]

  # Calculate mean and median consumption in Belgium
  print(np.mean(be_consumption["consumption"]))
  print(np.median(be_consumption["consumption"]))

  # Calculate mean and median consumption in USA
  print(np.mean(usa_consumption["consumption"]))
  print(np.median(usa_consumption["consumption"]))
  
   
  # Import numpy as np
  import numpy as np

  # Subset for Belgium and USA only
  be_and_usa = food_consumption[(food_consumption["country"]=="Belgium") | (food_consumption["country"]=="USA")]

  # Group by country, select consumption column, and compute mean and median
  print(be_and_usa.groupby(be_and_usa["country"])["consumption"].agg([np.mean,np.median]))
  
  import matplotlib.pyplot as plt

  # Subset for food_category equals rice
  rice_consumption = food_consumption[food_consumption['food_category'] == 'rice']

  # Histogram of co2_emission for rice and show plot
  rice_consumption.co2_emission.hist()
  plt.show()
  
  
  ##Measures of center
  #median
  np.median(msleep["sleep_total"]
  
  #mode
  import statistics
  statistics.mode(msleep["vore'])
  
  #adding an outlier
  msleep[msleep['vore'] == 'insecti']['sleep_total'].agg([np.mean, np.median])
  
  
  
  #Mean and Median
  # Import numpy with alias np
  import numpy as np

  # Filter for Belgium
  be_consumption = food_consumption[food_consumption["country"] == 'Belgium']

  # Filter for USA
  usa_consumption = food_consumption[food_consumption["country"] == "USA"]

  # Calculate mean and median consumption in Belgium
  print(np.mean(be_consumption["consumption"]))
  print(np.median(be_consumption["consumption"]))

  # Calculate mean and median consumption in USA
  print(np.mean(usa_consumption["consumption"]))
  print(np.median(usa_consumption["consumption"]))
  
  
  
  # Import numpy as np
  import numpy as np

  # Subset for Belgium and USA only
  be_and_usa = food_consumption[(food_consumption["country"]=="Belgium") | (food_consumption["country"]=="USA")]

  # Group by country, select consumption column, and compute mean and median
  print(be_and_usa.groupby(be_and_usa["country"])["consumption"].agg([np.mean,np.median]))
  
  # Calculate mean and median of co2_emission with .agg()
  print(rice_consumption.agg([np.mean, np.median]))
  
  
  
  #Measures of spread
  #Calculating variance
  #1.substract mean from each data point
  dists = msleep['sleep_total']-np.mean(msleep['sleep_total'])
  print(dists)
  
  #2. Square each distance
  sq_dists = dists **2
  print(sq_dists)
  
  #3. sum squared distances
  sum_sq_dists = np.sum(sq.dists)
  print(sum_sq_dists)
  
  #4. divide by number of data points -1
  variance = sum_sq_dists/ (83-1)
  print(variance)
  
  ##calculate in 1 step
  np.var(msleep['sleep_total'], ddof=1)
  
  #without ddof=1, population variance is calculated instead of sample variance:
  np.var(msleep['sleep_total'])
  
  
  
  #Standard Deviation
  np.sqrt(np.var(msleep['sleep_total'], ddof =1))
  
  np.std(msleep['sleep_total'], ddof=1)
  
  
  #Mean absolute deviation 
  dists= msleep['sleep_total'] - mean(msleep$sleep_total)
  np.mean(np.abs(dists))
  
  
  #Quantiles
  np.quantile(msleep['sleep_total'], 0.5)
  0.5 quantile = median
  
  np.quantile(msleep['sleep_total'],[0,0.25,0.5,0.75,1])
  
  #boxplot use quantiles
  import matplotlib.pyplot as plt
  plt.boxplot(msleep['sleep_total'])
  plt.show()
  
  #Quantiles using np.linspace()
  np.quantile(msleep['sleep_total'],[0,0.2,0.4,0.6,0.8,1])
  
  np.linspace(start, stop, num)
  np.quantile(msleep['sleep_total'], np.linspace(0,1,5))
  
  #Interquartile range (IQR)
  np.quantile(msleep['sleep_total'], 0.75) - np.quantile(msleep['sleep_total'], 0.25)
  
  from scipy.stats import iqr
  iqr(msleep['sleep_total'])
  
  
  #Outliers data>Q1 -1.5 x IQR or data <Q3 +1.5 xIQR
  #find outliers
  from scipy.stats import iqr
  iqr = iqr(msleep['bodywt'])
  lower_threshold = np.quantile(msleep['bodywt'], 0.25) -1.5 *iqr
  upper_threshold = np.quantile(msleep['bodywt'], 0.75) -1.5 *iqr
  
  msleep[(msleep['bodywt']) < lower_threshold) |(msleep['bodywt']) > upper_threshold)]
  
  
  #All in one go
  msleep['bodywt'].describe()
  
  
  #Quartiles, quantiles, and quintiles
  # Calculate the quartiles of co2_emission
  print(np.quantile(food_consumption["co2_emission"],np.linspace(0,1,5)))
  
  # Calculate the quintiles of co2_emission
  print(np.quantile(food_consumption["co2_emission"], np.linspace(0,1,6)))
  
 
  
  
  # Print variance and sd of co2_emission for each food_category
  print(food_consumption.groupby("food_category")["co2_emission"].agg([np.var, np.std]))

  # Import matplotlib.pyplot with alias plt
  import matplotlib.pyplot as plt

  # Create histogram of co2_emission for food_category 'beef'
  food_consumption[food_consumption["food_category"] == "beef"].co2_emission.hist()
  # Show plot
  plt.show()

  # Create histogram of co2_emission for food_category 'eggs'
  food_consumption[food_consumption["food_category"] == "eggs"].co2_emission.hist()
  # Show plot
  plt.show()
  
  
  
  # Calculate total co2_emission per country: emissions_by_country
  emissions_by_country = food_consumption.groupby("country")["co2_emission"].sum()
  print(emissions_by_country)
   

  # Compute the first and third quartiles and IQR of emissions_by_country
  q1 = np.quantile(emissions_by_country, 0.25)
  q3 = np.quantile(emissions_by_country, 0.75) 
  iqr = q3-q1
  
  # Calculate the lower and upper cutoffs for outliers
  lower = np.quantile(emissions_by_country, 0.25) - 1.5* iqr
  upper = np.quantile(emissions_by_country, 0.75) + 1.5* iqr
  **OR
  # Calculate the lower and upper cutoffs for outliers
  lower = q1 - 1.5 * iqr
  upper = q3 + 1.5 * iqr

  # Subset emissions_by_country to find outliers
  outliers = emissions_by_country[(emissions_by_country < lower) | (emissions_by_country >upper)]
  print(outliers)
  
  
 #Calculating probabilities
 # Count the deals for each product
 counts = amir_deals['product'].value_counts()
 print(counts)
 # Calculate probability of picking a deal with each product
 probs = counts/len(amir_deals["product"])
 print(probs)
 
 
  
 # Set random seed
 np.random.seed(24)

 # Sample 5 deals without replacement
 sample_without_replacement = amir_deals.sample(5, replace = False)
 print(sample_without_replacement)
 
  # Sample 5 deals with replacement
 sample_with_replacement = amir_deals.sample(5, replace = True)
 print(sample_with_replacement)
 
 
 ##Creating a probability distribution 
 # Create probability distribution
 size_dist = restaurant_groups['group_size'] / restaurant_groups.shape[0]

 # Reset index and rename columns
 size_dist = size_dist.reset_index()
 size_dist.columns = ['group_size', 'prob']
 print(size_dist)
 
 # Calculate expected value
 expected_value = (size_dist["group_size"] * size_dist["prob"]).sum()
 print(expected_value)
 
 # Subset groups of size 4 or more
 groups_4_or_more = size_dist[size_dist["group_size"]>= 4]

 # Sum the probabilities of groups_4_or_more
 prob_4_or_more = groups_4_or_more['prob'].sum()
 print(prob_4_or_more)
 
 
 
 ###Continuous distribution
 ##Uniform distribution in python
 from scipy.stats import uniform
 uniform.cdf(7, 0 ,12)   #waiting time less than or equal to 7 mins, 0 is lower and 12 is upper limit
 
 #waiting more than 7 mins
 1- uniform.cdf(7,0,12)
 
 #to generate random numbers according to uniform distribution
 from scipy.stats import uniform
 uniform.rvs(0,5, size = 10)
 
 
 #Data back-ups
 # Min and max wait times for back-up that happens every 30 min
 min_time = 0
 max_time = 30

 # Import uniform from scipy.stats
 from scipy.stats import uniform

 # Calculate probability of waiting less than 5 mins
 prob_less_than_5 = uniform.cdf(5,0,30)
 print(prob_less_than_5)
 
 # Calculate probability of waiting more than 5 mins
 prob_greater_than_5 = 1 - uniform.cdf(5,0,30)
 print(prob_greater_than_5)
 
 # Calculate probability of waiting 10-20 mins
 prob_between_10_and_20 = uniform.cdf(20,0,30) - uniform.cdf(10,0,30)
 print(prob_between_10_and_20)
 
 
 ##Simulating wait times
 # Set random seed to 334
 np.random.seed(334)
 
 # Import uniform
 from scipy.stats import uniform
 
 # Generate 1000 wait times between 0 and 30 mins
 wait_times = uniform.rvs(0,30, size =1000)
 print(wait_times)
 
 # Create a histogram of simulated times and show plot
 plt.hist(wait_times)
 plt.show()
 
 ##Distribution of Amir's sales
 amir_deals['amount'].hist(bins=10);
  
 #What's the probability of Amir closing a deal worth less than $7500?
 # Probability of deal < 7500
 prob_less_7500 = norm.cdf(7500, 5000, 2000)

 #What's the probability of Amir closing a deal worth more than $1000?
 # Probability of deal > 1000
 prob_over_1000 = 1- (norm.cdf(1000, 5000, 2000))

 #What's the probability of Amir closing a deal worth between $3000 and $7000?
 # Probability of deal between 3000 and 7000
 prob_3000_to_7000 = norm.cdf(7000,5000,2000) - norm.cdf(3000, 5000, 2000)
 
 #What amount will 25% of Amir's sales be less than?
 # Calculate amount that 25% of deals will be less than
 pct_25 = norm.ppf(0.25, 5000,2000)
 
 
 # Calculate new average amount
 new_mean = 5000 * 1.2

 # Calculate new standard deviation
 new_sd = 2000 * 1.3

 # Simulate 36 new sales
 new_sales = norm.rvs(new_mean, new_sd,36)

 # Create histogram and show
 plt.hist(new_sales)
 plt.show()
 
 
 
 ##The Central Limit theorem
 die = pd.Series([1,2,3,4,5,6])
 #Roll 5 times
 samp_5 = die.sample(5, replace = True)
 print(samp_5)
 np.mean(samp_5)
 
 
 ##Rolling the dice5 times 10 times
 Roll 5 times > take the mean > repeat 10 times
 sample_means = []
 for i in range(10):
  samp_5 = die.sample(5, replace = True)
  sample_means.append(np.mean(samp_5))
  print(sample_means)
 
 
 100 sample means
 sample_means = []
 for i in range(100):
  sample_means.append(np.mean(die.sample(5, replace = True)))
  
 
 
 ##Proportions and the CLT
 sales_team = pd.Series(["Amir", "Brian", "Claire", "Damian"])
 sales_team.sample(10, replace = True)
 
 #The CLT in action
 # Create a histogram of num_users and show
 amir_deals["num_users"].hist()
 plt.show() 
 
 # Set seed to 104
 np.random.seed(104)

 # Sample 20 num_users with replacement from amir_deals
 samp_20 = amir_deals["num_users"].sample(20, replace=True)

 # Take mean of samp_20
 print(np.mean(samp_20))
 
 
 sample_means = []
 # Loop 100 times
 for i in range(100):
   # Take sample of 20 num_users
   samp_20 = amir_deals['num_users'].sample(20, replace=True)
   # Calculate mean of samp_20
   samp_20_mean = np.mean(samp_20)
   # Append samp_20_mean to sample_means
   sample_means.append(samp_20_mean)
 print(sample_means)
 
 # Convert to Series and plot histogram
 sample_means_series = pd.Series(sample_means)
 sample_means_series.hist()
 # Show plot
 plt.show()
 
 
 
 ##The mean of means
 # Set seed to 321
 np.random.seed(321)

 sample_means = []
 # Loop 30 times to take 30 means
 for i in range(30):
   # Take sample of size 20 from num_users col of all_deals with replacement
   cur_sample = all_deals["num_users"].sample(20, replace = True)
   # Take mean of cur_sample
   cur_mean = np.mean(cur_sample)
   # Append cur_mean to sample_means
   sample_means.append(cur_mean)

 # Print mean of sample_means
 print(np.mean(sample_means))

 # Print mean of num_users in amir_deals
 print(np.mean(amir_deals["num_users"]))
 
 
 
 
 
 
  
